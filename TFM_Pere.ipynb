{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TFM_Pere.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "rol3jSCYx_Oe"
      ],
      "mount_file_id": "1EZEVpUEKNegZiAmuxRoDXseTyMeU4qHC",
      "authorship_tag": "ABX9TyPOLOJ1VVXmQNhMl7EFa/47",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pfescriva/Applied-AI-for-Business-Insight/blob/main/TFM_Pere.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJUgKQEKpzRp"
      },
      "source": [
        "# Code to load libraries:\n",
        "\n",
        "# !pip install mglearn\n",
        "# !pip install gensim\n",
        "# !pip install stop_words\n",
        "# !pip install pyLDAvis\n",
        "# !pip install langdetect\n",
        "# !pip install unidecode\n",
        "\n",
        "import sklearn as sk\n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "import mglearn\n",
        "from stop_words import get_stop_words\n",
        "import nltk, re, string, collections\n",
        "from nltk.util import ngrams # function for making ngrams\n",
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.datasets import make_multilabel_classification\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import pyLDAvis\n",
        "import pyLDAvis.sklearn\n",
        "import warnings\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim import utils \n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
        "import matplotlib.pyplot as plt\n",
        "from langdetect import detect\n",
        "import spacy\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models.ldamulticore import LdaMulticore\n",
        "from gensim.models.coherencemodel import CoherenceModel\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tV-VlY2xv4wO"
      },
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "path = '/content/drive/MyDrive/TFM data/urjc.jsonl.gz'\n",
        "data = pd.read_json(path, lines = True)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8BjIduV4gLg"
      },
      "source": [
        "\n",
        "# 0. Some initial cleaning (Empty cols and constant cols)\n",
        "data.dropna(how = 'all', axis = 1, inplace = True) # Drop empty columns\n",
        "data = data.loc[:, (data != data.iloc[0]).any()]  # Drop Constant columns\n",
        "print(data.shape[0])\n",
        "\n",
        "# Filter out non-spanish documents\n",
        "data = data.loc[(data['lang'] == \"es\")].reset_index(drop  = True)\n",
        "print('First lang filter: ' + str(data.shape[0]))\n",
        "\n",
        "# Filter out non-spanish documents with higher precission.\n",
        "data['Language'] = data['text'].apply(detect)\n",
        "data = data.loc[(data['Language'] == \"es\")].reset_index(drop  = True)\n",
        "print('First lang filter: ' + str(data.shape[0]))\n",
        "\n",
        "# Keep only non RT information\n",
        "data = data.loc[ ~ (data['text'].str.startswith('RT '))]\n",
        "print('RT removal: ' + str(data.shape[0]))\n",
        "\n",
        "# 1. Drop duplicated documents that are writen by the same user, and laso drop any tweet that is NA (There were not anyway)\n",
        "data = data.drop_duplicates(subset = ['author_id', 'text'], keep = 'first', inplace = False, ignore_index = False).reset_index(drop = True)\n",
        "data = data.dropna(subset = ['text'])\n",
        "print('Dropped duplicates and empty texts: ' + str(data.shape[0]))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XowDVZ4t4gOV"
      },
      "source": [
        "data.to_pickle(\"clean_data.pkl\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWCfJdWB43OL"
      },
      "source": [
        "# data = pd.read_pickle(\"clean_data.pkl\")\n",
        "data = data[data['in_reply_to_user_id'].isnull()]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96SGmPch43RF"
      },
      "source": [
        "\n",
        "import os\n",
        "os.getcwd()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2cwliSvm5AiM",
        "outputId": "fcf54821-eb27-4dfb-c2ac-1746481a872f"
      },
      "source": [
        "\n",
        "\"\"\"\n",
        "\n",
        "PRE-PROCESS: \n",
        "\n",
        "0. Get the unduplicated / non-retweeted tweets. \n",
        "\n",
        "\n",
        "NON-TUNED PRE-PROCESSING STEPS:\n",
        "\n",
        "1. Remove the hashtags, links and shares\n",
        "\n",
        "2. Lowcase \n",
        "\n",
        "3. Remove punctuation \n",
        "\n",
        "4. Remove accents \n",
        "\n",
        "5. Remove numbers (TBC)\n",
        "\n",
        "6. Lemmatise \n",
        "\n",
        "7. Make some corrections uncovered above\n",
        "\n",
        "\n",
        "TUNED STEPS:\n",
        "\n",
        "8. Remove stopwords (TBC properly) \n",
        "\n",
        "9. Remove short tweets \n",
        "\n",
        "10. Group by author and gridsearch best LDA hyperparameters.  \n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# 1. Remove links, hashtags and mentions\n",
        "# --------------------------------------\n",
        "\n",
        "def hashtag(txt):\n",
        "    \n",
        "    # Remove hashtags\n",
        "    result = ' '.join(word for word in txt.split(' ') if not word.startswith('#'))\n",
        "    \n",
        "    # Remove shares\n",
        "    result = ' '.join(word for word in result.split(' ') if not word.startswith('@'))\n",
        "    # check if this doesn't work well use: result = re.sub(r'@\\S+', '', result)\n",
        "        \n",
        "    # Remove some jajajas\n",
        "    result = ' '.join(word for word in result.split(' ') if 'jaja' not in word.lower())\n",
        "    result = ' '.join(word for word in result.split(' ') if 'jj' not in word.lower())\n",
        "    result = ' '.join(word for word in result.split(' ') if 'haha' not in word.lower())\n",
        "\n",
        "    # Remove links\n",
        "    result = re.sub(r'http\\S+', '', result)\n",
        "\n",
        "    # Further work with potential remainings\n",
        "    result = re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''', \" \", result)\n",
        "    result = re.sub(r'[¿“‘?’\"„“<>,!\"]', \"\", result)\n",
        "    result = re.sub('!', \"\", result)\n",
        "    \n",
        "    return result\n",
        "\n",
        "    \n",
        "data['text'] = data['text'].apply(hashtag)\n",
        "\n",
        "\n",
        "# 2. Lowcase \n",
        "# --------------------------------------\n",
        "\n",
        "data['text'] = data['text'].apply(np.char.lower)\n",
        "\n",
        "\n",
        "\n",
        "# 3. Remove punctuation from text\n",
        "# --------------------------------------\n",
        "\n",
        "from gensim.parsing.preprocessing import strip_punctuation\n",
        "data['text'] = data['text'].apply(strip_punctuation)\n",
        "\n",
        "\n",
        "\n",
        "# 4. Remove accents from text\n",
        "# --------------------------------------\n",
        "\n",
        "import unidecode\n",
        "data['text'] = data['text'].apply(unidecode.unidecode)\n",
        "data['text'] = data['text'].apply(gensim.utils.deaccent) # In case we apply both \n",
        "\n",
        "\n",
        "\n",
        "# 5. Remove numbers\n",
        "# --------------------------------------\n",
        "\n",
        "data['text'] = data['text'].str.replace('\\d+', '')\n",
        "\n",
        "\n",
        "\n",
        "# 6. Lemmatisation\n",
        "# --------------------------------------\n",
        "# The purpose of this code is to lemmatize (Convert to dictionary form) the words in each document, \n",
        "# keeping the same format\n",
        "\n",
        "nlp = spacy.load('es_core_news_sm', exclude = ['derechos', 'expres', 'ademas', 'traves', 'adios'], disable = ['derechos', 'expres', 'ademas', 'traves', 'adios'])\n",
        "\n",
        "def lemmatizer(text):  \n",
        "  doc = nlp(text)\n",
        "  return ' '.join([word.lemma_ for word in doc])\n",
        "\n",
        "data['lemmatized'] = data['text'].apply(lambda x: lemmatizer(x)) \n",
        "\n",
        "\n",
        "\n",
        "# 7. Further cleaning\n",
        "# --------------------------------------\n",
        "\n",
        "def corrector(text):     \n",
        "    text = text.replace(' lumnos ', ' alumno ') \n",
        "    text = text.replace(' estudiante ', ' alumno ') # Synonims I think it's worth having them as the same word\n",
        "    text = text.replace(' avda ', ' avenida ')\n",
        "    text = text.replace(' info ', ' informacion ')\n",
        "    text = text.replace(' uni ', ' universidad ')\n",
        "    text = text.replace(' almerio ', ' almeria ')\n",
        "    text = text.replace(' estudio ', ' estudiar ')\n",
        "    text = text.replace(' cambier ', ' cambiar ')\n",
        "    \n",
        "    return text\n",
        "\n",
        "data['lemmatized'] = data['lemmatized'].apply(corrector)\n",
        "\n",
        "from gensim.parsing.preprocessing import strip_multiple_whitespaces\n",
        "data['lemmatized'] = data['lemmatized'].apply(strip_multiple_whitespaces)\n",
        "\n",
        "\n",
        "\n",
        "# 8. Stopwords work \n",
        "# --------------------------------------\n",
        "\n",
        "# Find the list of stopwords\n",
        "\n",
        "# Note that the removal depends on the library and will be part of each. \n",
        "stop_words = stopwords.words('spanish')\n",
        "stop_words_extension = get_stop_words('es')\n",
        "stop_words.extend(stop_words_extension)\n",
        "\n",
        "# Remove accents from stopwords (I'll work with no accents anywhere, since we don't expect people to write always with accents) \n",
        "stop_words = [gensim.utils.deaccent(each_word) for each_word in stop_words]\n",
        "\n",
        "stop_words.extend(['interesante', 'universidadjuancarlos', 'él', 'hala', 'juanca', 'reyjuancar', 'urjcritica', 'this', 'was', 'date', 'great', 'my', 'first', 'claro', 'queydondeestudiar2020', 'rey', 'juan', 'literalmente', 'universidadreyjuancarlo', 'universidadjuancarlos', 'lareyjuancarlos', 'universidadreyjuancarlos', 'universidadreyjuancarlo', 'urjc', 'gracia', 'gracias','ajjaj', 'xd', 'xdd', 'xdxd', 'Madrid', 'primero', 'segundo', 'tercero', 'cuarto', 'quinto', 'sexto', 'septimo', 'octavo', 't', 'a', 's', 'k', 'q', 'mas', 'tambien', 'ir', 'alla', 'cosa', 'iee', 'siquiera', 'in', 'the', 'of', 'ano', 'n', 'p', 'r', 'asi', 'coincidir', 'aa', 'decir', '¡', '¿', 'poder', 'podeis', 'estais', 'traves', 'alguno', 'buen', 'nosotrxs', 'bufff', 'buff', 'universitario', 'lol', 'omg', 'wtf', 'idk', 'fyi', 'tbh', 'lmao', 'asap', 'thanks', 'thank', 'thx', \n",
        "                'trav', 'bastante', 'muchisimo', 'muchisimos', 'muchisima', 'muchisimas', 'muchisimar', 'monton', 'habia', 'xe', 'jo', 'ops', 'ups', 'yupi', 'poca', 'poco', 'enhorabuena', 'incluso', 'igual', 'ahora', 'despu', 'pese', 'ser', 'cierto', 'haber', 'for', 'par', 'universidad', 'with', 'asimismo', 'carlos', 'universidad', 'iii', 'ser', 'entonces', 'madrid', 'tras', 'jo él', 'hacer', 'espán', 'noticia', 'as', 'periodico', 'diario', 'news', 'si', 'aca', 'mas', 'ademas', 'gracias', 'aqui', 'hola', 'saludos', 'hoy', 'co', 'bien', 'ver', 'dar', 'vosotrxs', 'wow', 'ahora', 'solo', 'todo', 'cualquier', 'cualquiera', 'pues', 'vez', 'mismo', 'evidentemente', 'uee', 'hey', 'hello', 'xvii', 'ii', 'iii', 'iv', 'v', 'vi', 'vii', 'viii', 'ix', 'x', 'xi', 'xii', 'xiii', 'xiv', 'xv', 'xvi', 'xviii', 'ixx', 'xx', 'xxi', \n",
        "                'parte', 'normalmente', 'mil', 'adio', 'millon', '!', 'yosoyurjc', 'haced', 'hacerte', 'mientras', 'menos', 'después', 'despues', 'dias', 'cuyo', 'cuya', 'of', '_', 'aun', 'nunca', 'siempre', 'muchisima', 'dia', 'algun', 'pon', 'sino', 'mejor', 'poca', 'peor', 'ano', 'nuevo', '200', '-', 'uee', 'href', 'uno', 'dos', 'tres', 'espán', 'medal él', 'desmontar él', 'jo él', 'joder', 'char él', 'queydondeestudiar', 'mer él', 'cuatro', 'cinco', 'seis', 'siete', 'ocho', 'nueve', 'diez', 'veintena', 'doce', 'docena', 'trece', 'veitiuno', 'cien', 'decena', 'x2', '–', 'https', 'http', 'rt', 'm', 'uc', 'alguien', 'cada', 'latest', 'tmb', 'via', 'tal', 'etc', 'etcetera', 'traves', 'ma', 'ahi', 'aqui', 'alli', 'alla', 'so', 'if', 'mmm', 'hmm', 'ja', 'cuidadosamente', 'quizas', 'quiza', 'nada', 'nunca', 'probablemente', 'mismisimo', 'totalmente', 'completamente'])\n",
        "\n",
        "stop_words.extend(['informacion', 'titulo', 'alumno', 'estudiar', 'profesor', 'primerisimo', 'adema', 'yomequedoencasa'])\n",
        "\n",
        "# Frequent words that add litle meaning: Hyperparameter in gridsearch\n",
        "extras = ['seguro', 'realizar', 'titulo', 'clase', 'tener', 'necesitar', 'querer', 'abrir', 'agarrar', 'andar', 'caminar', 'buscar', 'caer', 'conocer', 'saber', 'hablar', 'tener', 'tomar', 'poner', 'dar', 'ir', 'decir', 'estar', 'ser', 'vivir', 'oir', 'poner', 'traer', 'sentir', 'ver', 'mirar', 'llegar', 'llevar', 'entender', 'oler', 'salir', 'comprender', 'rayar', 'tener', 'flipar', 'putear', 'darle', 'saber', 'pasar', 'poner', 'esperar', 'dejar', 'parecer', 'salir', 'seguir', 'creer', 'opinar', 'informacion', 'alumno']\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<input>:95: DeprecationWarning: invalid escape sequence \\d\n",
            "<input>:95: DeprecationWarning: invalid escape sequence \\d\n",
            "<input>:95: DeprecationWarning: invalid escape sequence \\d\n",
            "<input>:95: DeprecationWarning: invalid escape sequence \\d\n",
            "<input>:95: DeprecationWarning: invalid escape sequence \\d\n",
            "<input>:95: DeprecationWarning: invalid escape sequence \\d\n",
            "<input>:95: DeprecationWarning: invalid escape sequence \\d\n",
            "<input>:95: DeprecationWarning: invalid escape sequence \\d\n",
            "<input>:95: DeprecationWarning: invalid escape sequence \\d\n",
            "<input>:95: DeprecationWarning: invalid escape sequence \\d\n",
            "<input>:95: DeprecationWarning: invalid escape sequence \\d\n",
            "<input>:95: DeprecationWarning: invalid escape sequence \\d\n",
            "<input>:95: DeprecationWarning: invalid escape sequence \\d\n",
            "<input>:95: DeprecationWarning: invalid escape sequence \\d\n",
            "<input>:95: DeprecationWarning: invalid escape sequence \\d\n",
            "<input>:95: DeprecationWarning: invalid escape sequence \\d\n",
            "<input>:95: DeprecationWarning: invalid escape sequence \\d\n",
            "<input>:95: DeprecationWarning: invalid escape sequence \\d\n",
            "<input>:95: DeprecationWarning: invalid escape sequence \\d\n",
            "<input>:95: DeprecationWarning: invalid escape sequence \\d\n",
            "<input>:95: DeprecationWarning: invalid escape sequence \\d\n",
            "<input>:95: DeprecationWarning: invalid escape sequence \\d\n",
            "<input>:95: DeprecationWarning: invalid escape sequence \\d\n",
            "<input>:95: DeprecationWarning: invalid escape sequence \\d\n",
            "<input>:95: DeprecationWarning: invalid escape sequence \\d\n",
            "<input>:95: DeprecationWarning: invalid escape sequence \\d\n",
            "<input>:95: DeprecationWarning: invalid escape sequence \\d\n",
            "<input>:95: DeprecationWarning: invalid escape sequence \\d\n",
            "<input>:95: DeprecationWarning: invalid escape sequence \\d\n",
            "<input>:95: DeprecationWarning: invalid escape sequence \\d\n",
            "<input>:95: DeprecationWarning: invalid escape sequence \\d\n",
            "<input>:95: DeprecationWarning: invalid escape sequence \\d\n",
            "<input>:95: DeprecationWarning: invalid escape sequence \\d\n",
            "<input>:95: DeprecationWarning: invalid escape sequence \\d\n",
            "<input>:95: DeprecationWarning: invalid escape sequence \\d\n",
            "<input>:95: DeprecationWarning: invalid escape sequence \\d\n",
            "<input>:95: DeprecationWarning: invalid escape sequence \\d\n",
            "<input>:95: DeprecationWarning: invalid escape sequence \\d\n",
            "<input>:95: DeprecationWarning: invalid escape sequence \\d\n",
            "<input>:95: DeprecationWarning: invalid escape sequence \\d\n",
            "<input>:95: DeprecationWarning: invalid escape sequence \\d\n",
            "<input>:95: DeprecationWarning: invalid escape sequence \\d\n",
            "<input>:95: DeprecationWarning: invalid escape sequence \\d\n",
            "<input>:95: DeprecationWarning: invalid escape sequence \\d\n",
            "<input>:95: DeprecationWarning: invalid escape sequence \\d\n",
            "<input>:95: DeprecationWarning: invalid escape sequence \\d\n",
            "<input>:95: DeprecationWarning: invalid escape sequence \\d\n",
            "<input>:95: DeprecationWarning: invalid escape sequence \\d\n",
            "<input>:95: DeprecationWarning: invalid escape sequence \\d\n",
            "<input>:95: DeprecationWarning: invalid escape sequence \\d\n",
            "<input>:95: DeprecationWarning: invalid escape sequence \\d\n",
            "<input>:95: DeprecationWarning: invalid escape sequence \\d\n",
            "<input>:95: DeprecationWarning: invalid escape sequence \\d\n",
            "<input>:95: DeprecationWarning: invalid escape sequence \\d\n",
            "<input>:95: DeprecationWarning: invalid escape sequence \\d\n",
            "<input>:95: DeprecationWarning: invalid escape sequence \\d\n",
            "<input>:95: DeprecationWarning: invalid escape sequence \\d\n",
            "<input>:95: DeprecationWarning: invalid escape sequence \\d\n",
            "<ipython-input-26-89c9e88a91df>:95: DeprecationWarning: invalid escape sequence \\d\n",
            "  data['text'] = data['text'].str.replace('\\d+', '')\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:95: FutureWarning: The default value of regex will change from True to False in a future version.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-89c9e88a91df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;31m# keeping the same format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'es_core_news_sm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'derechos'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'expres'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ademas'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'traves'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'adios'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'derechos'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'expres'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ademas'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'traves'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'adios'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mdeprecation_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exists\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Path or Path-like to model data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'es_core_news_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESiJ4zOZ5Ak_",
        "outputId": "6c6e5657-3d01-4805-e404-42f38d0fed9f"
      },
      "source": [
        "# spacy.load('es_core_news_sm')\n",
        "\n",
        "!python -m spacy download es_core_news_sm"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting es_core_news_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-2.2.5/es_core_news_sm-2.2.5.tar.gz (16.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 16.2 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from es_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (4.62.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.21.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_sm==2.2.5) (4.6.4)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_sm==2.2.5) (3.5.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (1.24.3)\n",
            "Building wheels for collected packages: es-core-news-sm\n",
            "  Building wheel for es-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for es-core-news-sm: filename=es_core_news_sm-2.2.5-py3-none-any.whl size=16172933 sha256=d5309bbd5dcefb01e96de7e65d80d0eedca80c54aa9937da351dc10a74676dd1\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ofxql430/wheels/21/8d/a9/6c1a2809c55dd22cd9644ae503a52ba6206b04aa57ba83a3d8\n",
            "Successfully built es-core-news-sm\n",
            "Installing collected packages: es-core-news-sm\n",
            "Successfully installed es-core-news-sm-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('es_core_news_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "qvNltyew5An5",
        "outputId": "2f7f57a5-07e9-4e3b-b0d4-0d0528c77ce0"
      },
      "source": [
        "import es_core_web_sm"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-f33f395161da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mes_core_web_sm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'es_core_web_sm'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O43Jv1q45ArB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Wg4qN9u43Tw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "AiP_BDzTweOd",
        "outputId": "7e56efda-4e2c-4432-87ab-d4313b326bde"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-8fd22cca3849>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/TFM data/urjc.jsonl.gz'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcapdou\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options)\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    744\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m                 \u001b[0mdata_lines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 746\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_combine_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_lines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    747\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_get_object_parser\u001b[0;34m(self, json)\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"frame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 770\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFrameParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    771\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"series\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_no_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_parse_no_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1138\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m             self.obj = DataFrame(\n\u001b[0;32m-> 1140\u001b[0;31m                 \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecise_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecise_float\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1141\u001b[0m             )\n\u001b[1;32m   1142\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"split\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rol3jSCYx_Oe"
      },
      "source": [
        "# Nueva sección"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrehI_JuqfHR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uy10O-yIwFSf"
      },
      "source": [
        "\n",
        "\"\"\"\n",
        "\n",
        "PRE-PROCESS: \n",
        "\n",
        "0. Get the unduplicated / non-retweeted tweets. \n",
        "\n",
        "\n",
        "NON-TUNED PRE-PROCESSING STEPS:\n",
        "\n",
        "1. Remove the hashtags, links and shares\n",
        "\n",
        "2. Lowcase \n",
        "\n",
        "3. Remove punctuation \n",
        "\n",
        "4. Remove accents \n",
        "\n",
        "5. Remove numbers (TBC)\n",
        "\n",
        "6. Lemmatise \n",
        "\n",
        "7. Make some corrections uncovered above\n",
        "\n",
        "\n",
        "TUNED STEPS:\n",
        "\n",
        "8. Remove stopwords (TBC properly) \n",
        "\n",
        "9. Remove short tweets \n",
        "\n",
        "10. Group by author and gridsearch best LDA hyperparameters.  \n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# 1. Remove links, hashtags and mentions\n",
        "# --------------------------------------\n",
        "\n",
        "def hashtag(txt):\n",
        "    \n",
        "    # Remove hashtags\n",
        "    result = ' '.join(word for word in txt.split(' ') if not word.startswith('#'))\n",
        "    \n",
        "    # Remove shares\n",
        "    result = ' '.join(word for word in result.split(' ') if not word.startswith('@'))\n",
        "    \n",
        "    # Remove some jajajas\n",
        "    result = ' '.join(word for word in result.split(' ') if 'jaja' not in word.lower())\n",
        "    result = ' '.join(word for word in result.split(' ') if 'jj' not in word.lower())\n",
        "    result = ' '.join(word for word in result.split(' ') if 'haha' not in word.lower())\n",
        "\n",
        "    # Remove links\n",
        "    result = re.sub(r'http\\S+', '', result)\n",
        "\n",
        "    # Further work with potential remainings\n",
        "    result = re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''', \" \", result)\n",
        "    result = re.sub(r'[¿“‘?’\"„“<>,!\"]', \"\", result)\n",
        "    result = re.sub('!', \"\", result)\n",
        "    \n",
        "    return result\n",
        "\n",
        "    \n",
        "data['text'] = data['text'].apply(hashtag)\n",
        "\n",
        "\n",
        "# 2. Lowcase \n",
        "# --------------------------------------\n",
        "\n",
        "data['text'] = data['text'].apply(np.char.lower)\n",
        "\n",
        "\n",
        "\n",
        "# 3. Remove punctuation from text\n",
        "# --------------------------------------\n",
        "\n",
        "from gensim.parsing.preprocessing import strip_punctuation\n",
        "data['text'] = data['text'].apply(strip_punctuation)\n",
        "\n",
        "\n",
        "\n",
        "# 4. Remove accents from text\n",
        "# --------------------------------------\n",
        "\n",
        "import unidecode\n",
        "data['text'] = data['text'].apply(unidecode.unidecode)\n",
        "data['text'] = data['text'].apply(gensim.utils.deaccent) # In case we apply both \n",
        "\n",
        "\n",
        "\n",
        "# 5. Remove numbers\n",
        "# --------------------------------------\n",
        "\n",
        "data['text'] = data['text'].str.replace('\\d+', '')\n",
        "\n",
        "\n",
        "\n",
        "# 6. Lemmatisation\n",
        "# --------------------------------------\n",
        "# The purpose of this code is to lemmatize (Convert to dictionary form) the words in each document, \n",
        "# keeping the same format\n",
        "\n",
        "nlp = spacy.load('es_core_news_sm', exclude = ['derechos', 'expres', 'ademas', 'traves', 'adios'], disable = ['derechos', 'expres', 'ademas', 'traves', 'adios'])\n",
        "\n",
        "def lemmatizer(text):  \n",
        "  doc = nlp(text)\n",
        "  return ' '.join([word.lemma_ for word in doc])\n",
        "\n",
        "data['lemmatized'] = data['text'].apply(lambda x: lemmatizer(x)) \n",
        "\n",
        "\n",
        "\n",
        "# 7. Further cleaning\n",
        "# --------------------------------------\n",
        "\n",
        "def corrector(text):     \n",
        "    text = text.replace(' lumnos ', ' alumno ') \n",
        "    text = text.replace(' estudiante ', ' alumno ') # Synonims I think it's worth having them as the same word\n",
        "    text = text.replace(' avda ', ' avenida ')\n",
        "    text = text.replace(' info ', ' informacion ')\n",
        "    text = text.replace(' uni ', ' universidad ')\n",
        "    text = text.replace(' almerio ', ' almeria ')\n",
        "    text = text.replace(' estudio ', ' estudiar ')\n",
        "    text = text.replace(' cambier ', ' cambiar ')\n",
        "    \n",
        "    return text\n",
        "\n",
        "data['lemmatized'] = data['lemmatized'].apply(corrector)\n",
        "\n",
        "from gensim.parsing.preprocessing import strip_multiple_whitespaces\n",
        "data['lemmatized'] = data['lemmatized'].apply(strip_multiple_whitespaces)\n",
        "\n",
        "\n",
        "\n",
        "# 8. Stopwords work \n",
        "# --------------------------------------\n",
        "\n",
        "# Find the list of stopwords\n",
        "\n",
        "# Note that the removal depends on the library and will be part of each. \n",
        "stop_words = stopwords.words('spanish')\n",
        "stop_words_extension = get_stop_words('es')\n",
        "stop_words.extend(stop_words_extension)\n",
        "\n",
        "# Remove accents from stopwords (I'll work with no accents anywhere, since we don't expect people to write always with accents) \n",
        "stop_words = [gensim.utils.deaccent(each_word) for each_word in stop_words]\n",
        "\n",
        "stop_words.extend(['interesante', 'universidadjuancarlos', 'él', 'hala', 'juanca', 'reyjuancar', 'urjcritica', 'this', 'was', 'date', 'great', 'my', 'first', 'claro', 'queydondeestudiar2020', 'rey', 'juan', 'literalmente', 'universidadreyjuancarlo', 'universidadjuancarlos', 'lareyjuancarlos', 'universidadreyjuancarlos', 'universidadreyjuancarlo', 'urjc', 'gracia', 'gracias','ajjaj', 'xd', 'xdd', 'xdxd', 'Madrid', 'primero', 'segundo', 'tercero', 'cuarto', 'quinto', 'sexto', 'septimo', 'octavo', 't', 'a', 's', 'k', 'q', 'mas', 'tambien', 'ir', 'alla', 'cosa', 'iee', 'siquiera', 'in', 'the', 'of', 'ano', 'n', 'p', 'r', 'asi', 'coincidir', 'aa', 'decir', '¡', '¿', 'poder', 'podeis', 'estais', 'traves', 'alguno', 'buen', 'nosotrxs', 'bufff', 'buff', 'universitario', 'lol', 'omg', 'wtf', 'idk', 'fyi', 'tbh', 'lmao', 'asap', 'thanks', 'thank', 'thx', \n",
        "                'trav', 'bastante', 'muchisimo', 'muchisimos', 'muchisima', 'muchisimas', 'muchisimar', 'monton', 'habia', 'xe', 'jo', 'ops', 'ups', 'yupi', 'poca', 'poco', 'enhorabuena', 'incluso', 'igual', 'ahora', 'despu', 'pese', 'ser', 'cierto', 'haber', 'for', 'par', 'universidad', 'with', 'asimismo', 'carlos', 'universidad', 'iii', 'ser', 'entonces', 'madrid', 'tras', 'jo él', 'hacer', 'espán', 'noticia', 'as', 'periodico', 'diario', 'news', 'si', 'aca', 'mas', 'ademas', 'gracias', 'aqui', 'hola', 'saludos', 'hoy', 'co', 'bien', 'ver', 'dar', 'vosotrxs', 'wow', 'ahora', 'solo', 'todo', 'cualquier', 'cualquiera', 'pues', 'vez', 'mismo', 'evidentemente', 'uee', 'hey', 'hello', 'xvii', 'ii', 'iii', 'iv', 'v', 'vi', 'vii', 'viii', 'ix', 'x', 'xi', 'xii', 'xiii', 'xiv', 'xv', 'xvi', 'xviii', 'ixx', 'xx', 'xxi', \n",
        "                'parte', 'normalmente', 'mil', 'adio', 'millon', '!', 'yosoyurjc', 'haced', 'hacerte', 'mientras', 'menos', 'después', 'despues', 'dias', 'cuyo', 'cuya', 'of', '_', 'aun', 'nunca', 'siempre', 'muchisima', 'dia', 'algun', 'pon', 'sino', 'mejor', 'poca', 'peor', 'ano', 'nuevo', '200', '-', 'uee', 'href', 'uno', 'dos', 'tres', 'espán', 'medal él', 'desmontar él', 'jo él', 'joder', 'char él', 'queydondeestudiar', 'mer él', 'cuatro', 'cinco', 'seis', 'siete', 'ocho', 'nueve', 'diez', 'veintena', 'doce', 'docena', 'trece', 'veitiuno', 'cien', 'decena', 'x2', '–', 'https', 'http', 'rt', 'm', 'uc', 'alguien', 'cada', 'latest', 'tmb', 'via', 'tal', 'etc', 'etcetera', 'traves', 'ma', 'ahi', 'aqui', 'alli', 'alla', 'so', 'if', 'mmm', 'hmm', 'ja', 'cuidadosamente', 'quizas', 'quiza', 'nada', 'nunca', 'probablemente', 'mismisimo', 'totalmente', 'completamente'])\n",
        "\n",
        "stop_words.extend(['informacion', 'titulo', 'alumno', 'estudiar', 'profesor', 'primerisimo', 'adema', 'yomequedoencasa'])\n",
        "\n",
        "# Frequent words that add litle meaning: Hyperparameter in gridsearch\n",
        "extras = ['seguro', 'realizar', 'titulo', 'clase', 'tener', 'necesitar', 'querer', 'abrir', 'agarrar', 'andar', 'caminar', 'buscar', 'caer', 'conocer', 'saber', 'hablar', 'tener', 'tomar', 'poner', 'dar', 'ir', 'decir', 'estar', 'ser', 'vivir', 'oir', 'poner', 'traer', 'sentir', 'ver', 'mirar', 'llegar', 'llevar', 'entender', 'oler', 'salir', 'comprender', 'rayar', 'tener', 'flipar', 'putear', 'darle', 'saber', 'pasar', 'poner', 'esperar', 'dejar', 'parecer', 'salir', 'seguir', 'creer', 'opinar', 'informacion', 'alumno']\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}