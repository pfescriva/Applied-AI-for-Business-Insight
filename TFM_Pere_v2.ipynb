{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TFM_Pere.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "rol3jSCYx_Oe"
      ],
      "mount_file_id": "1EZEVpUEKNegZiAmuxRoDXseTyMeU4qHC",
      "authorship_tag": "ABX9TyPaVpo7e9hopFg5WjmntHe2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pfescriva/Applied-AI-for-Business-Insight/blob/main/TFM_Pere_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJUgKQEKpzRp"
      },
      "source": [
        "# Code to load libraries:\n",
        "\n",
        "# !pip install mglearn\n",
        "# !pip install gensim\n",
        "# !pip install stop_words\n",
        "# !pip install pyLDAvis\n",
        "# !pip install langdetect\n",
        "\n",
        "import sklearn as sk\n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "import mglearn\n",
        "from stop_words import get_stop_words\n",
        "import nltk, re, string, collections\n",
        "from nltk.util import ngrams # function for making ngrams\n",
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.datasets import make_multilabel_classification\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import pyLDAvis\n",
        "import pyLDAvis.sklearn\n",
        "import warnings\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim import utils \n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
        "import matplotlib.pyplot as plt\n",
        "from langdetect import detect\n",
        "import spacy\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models.ldamulticore import LdaMulticore\n",
        "from gensim.models.coherencemodel import CoherenceModel\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tV-VlY2xv4wO"
      },
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "path = '/content/drive/MyDrive/TFM data/urjc.jsonl.gz'\n",
        "data = pd.read_json(path, lines = True)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AiP_BDzTweOd",
        "outputId": "1a457d29-3cc3-4b89-f19f-884051c7033e"
      },
      "source": [
        "\n",
        "# 0. Some initial cleaning (Empty cols and constant cols)\n",
        "data.dropna(how = 'all', axis = 1, inplace = True) # Drop empty columns\n",
        "data = data.loc[:, (data != data.iloc[0]).any()]  # Drop Constant columns\n",
        "print(data.shape[0])\n",
        "\n",
        "data = data.loc[(data['lang'] == \"es\")].reset_index(drop  = True)\n",
        "print('First lang filter: ' + str(data.shape[0]))\n",
        "\n",
        "# Filter out non-spanish docuemnts with higher precission.\n",
        "data['Language'] = data['text'].apply(detect)\n",
        "data = data.loc[(data['Language'] == \"es\")].reset_index(drop  = True)\n",
        "print('First lang filter: ' + str(data.shape[0]))\n",
        "\n",
        "# Keep only non RT information\n",
        "data = data.loc[ ~ (data['text'].str.startswith('RT '))]\n",
        "print('RT removal: ' + str(data.shape[0]))\n",
        "\n",
        "# 1. Drop duplicated documents that are writen by the same user, and laso drop any tweet that is NA (There were not anyway)\n",
        "data = data.drop_duplicates(subset = ['author_id', 'text'], keep = 'first', inplace = False, ignore_index = False).reset_index(drop = True)\n",
        "data = data.dropna(subset = ['text'])\n",
        "print('Dropped duplicates and empty texts: ' + str(data.shape[0]))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "171294\n",
            "162192\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rol3jSCYx_Oe"
      },
      "source": [
        "# Nueva sección"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrehI_JuqfHR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uy10O-yIwFSf"
      },
      "source": [
        "\n",
        "\"\"\"\n",
        "\n",
        "PRE-PROCESS: \n",
        "\n",
        "0. Get the unduplicated / non-retweeted tweets. \n",
        "\n",
        "\n",
        "NON-TUNED PRE-PROCESSING STEPS:\n",
        "\n",
        "1. Remove the hashtags, links and shares\n",
        "\n",
        "2. Lowcase \n",
        "\n",
        "3. Remove punctuation \n",
        "\n",
        "4. Remove accents \n",
        "\n",
        "5. Remove numbers (TBC)\n",
        "\n",
        "6. Lemmatise \n",
        "\n",
        "7. Make some corrections uncovered above\n",
        "\n",
        "\n",
        "TUNED STEPS:\n",
        "\n",
        "8. Remove stopwords (TBC properly) \n",
        "\n",
        "9. Remove short tweets \n",
        "\n",
        "10. Group by author and gridsearch best LDA hyperparameters.  \n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# 1. Remove links, hashtags and mentions\n",
        "# --------------------------------------\n",
        "\n",
        "def hashtag(txt):\n",
        "    \n",
        "    # Remove hashtags\n",
        "    result = ' '.join(word for word in txt.split(' ') if not word.startswith('#'))\n",
        "    \n",
        "    # Remove shares\n",
        "    result = ' '.join(word for word in result.split(' ') if not word.startswith('@'))\n",
        "    \n",
        "    # Remove some jajajas\n",
        "    result = ' '.join(word for word in result.split(' ') if 'jaja' not in word.lower())\n",
        "    result = ' '.join(word for word in result.split(' ') if 'jj' not in word.lower())\n",
        "    result = ' '.join(word for word in result.split(' ') if 'haha' not in word.lower())\n",
        "\n",
        "    # Remove links\n",
        "    result = re.sub(r'http\\S+', '', result)\n",
        "\n",
        "    # Further work with potential remainings\n",
        "    result = re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''', \" \", result)\n",
        "    result = re.sub(r'[¿“‘?’\"„“<>,!\"]', \"\", result)\n",
        "    result = re.sub('!', \"\", result)\n",
        "    \n",
        "    return result\n",
        "\n",
        "    \n",
        "data['text'] = data['text'].apply(hashtag)\n",
        "\n",
        "\n",
        "# 2. Lowcase \n",
        "# --------------------------------------\n",
        "\n",
        "data['text'] = data['text'].apply(np.char.lower)\n",
        "\n",
        "\n",
        "\n",
        "# 3. Remove punctuation from text\n",
        "# --------------------------------------\n",
        "\n",
        "from gensim.parsing.preprocessing import strip_punctuation\n",
        "data['text'] = data['text'].apply(strip_punctuation)\n",
        "\n",
        "\n",
        "\n",
        "# 4. Remove accents from text\n",
        "# --------------------------------------\n",
        "\n",
        "import unidecode\n",
        "data['text'] = data['text'].apply(unidecode.unidecode)\n",
        "data['text'] = data['text'].apply(gensim.utils.deaccent) # In case we apply both \n",
        "\n",
        "\n",
        "\n",
        "# 5. Remove numbers\n",
        "# --------------------------------------\n",
        "\n",
        "data['text'] = data['text'].str.replace('\\d+', '')\n",
        "\n",
        "\n",
        "\n",
        "# 6. Lemmatisation\n",
        "# --------------------------------------\n",
        "# The purpose of this code is to lemmatize (Convert to dictionary form) the words in each document, \n",
        "# keeping the same format\n",
        "\n",
        "nlp = spacy.load('es_core_news_sm', exclude = ['derechos', 'expres', 'ademas', 'traves', 'adios'], disable = ['derechos', 'expres', 'ademas', 'traves', 'adios'])\n",
        "\n",
        "def lemmatizer(text):  \n",
        "  doc = nlp(text)\n",
        "  return ' '.join([word.lemma_ for word in doc])\n",
        "\n",
        "data['lemmatized'] = data['text'].apply(lambda x: lemmatizer(x)) \n",
        "\n",
        "\n",
        "\n",
        "# 7. Further cleaning\n",
        "# --------------------------------------\n",
        "\n",
        "def corrector(text):     \n",
        "    text = text.replace(' lumnos ', ' alumno ') \n",
        "    text = text.replace(' estudiante ', ' alumno ') # Synonims I think it's worth having them as the same word\n",
        "    text = text.replace(' avda ', ' avenida ')\n",
        "    text = text.replace(' info ', ' informacion ')\n",
        "    text = text.replace(' uni ', ' universidad ')\n",
        "    text = text.replace(' almerio ', ' almeria ')\n",
        "    text = text.replace(' estudio ', ' estudiar ')\n",
        "    text = text.replace(' cambier ', ' cambiar ')\n",
        "    \n",
        "    return text\n",
        "\n",
        "data['lemmatized'] = data['lemmatized'].apply(corrector)\n",
        "\n",
        "from gensim.parsing.preprocessing import strip_multiple_whitespaces\n",
        "data['lemmatized'] = data['lemmatized'].apply(strip_multiple_whitespaces)\n",
        "\n",
        "\n",
        "\n",
        "# 8. Stopwords work \n",
        "# --------------------------------------\n",
        "\n",
        "# Find the list of stopwords\n",
        "\n",
        "# Note that the removal depends on the library and will be part of each. \n",
        "stop_words = stopwords.words('spanish')\n",
        "stop_words_extension = get_stop_words('es')\n",
        "stop_words.extend(stop_words_extension)\n",
        "\n",
        "# Remove accents from stopwords (I'll work with no accents anywhere, since we don't expect people to write always with accents) \n",
        "stop_words = [gensim.utils.deaccent(each_word) for each_word in stop_words]\n",
        "\n",
        "stop_words.extend(['interesante', 'universidadjuancarlos', 'él', 'hala', 'juanca', 'reyjuancar', 'urjcritica', 'this', 'was', 'date', 'great', 'my', 'first', 'claro', 'queydondeestudiar2020', 'rey', 'juan', 'literalmente', 'universidadreyjuancarlo', 'universidadjuancarlos', 'lareyjuancarlos', 'universidadreyjuancarlos', 'universidadreyjuancarlo', 'urjc', 'gracia', 'gracias','ajjaj', 'xd', 'xdd', 'xdxd', 'Madrid', 'primero', 'segundo', 'tercero', 'cuarto', 'quinto', 'sexto', 'septimo', 'octavo', 't', 'a', 's', 'k', 'q', 'mas', 'tambien', 'ir', 'alla', 'cosa', 'iee', 'siquiera', 'in', 'the', 'of', 'ano', 'n', 'p', 'r', 'asi', 'coincidir', 'aa', 'decir', '¡', '¿', 'poder', 'podeis', 'estais', 'traves', 'alguno', 'buen', 'nosotrxs', 'bufff', 'buff', 'universitario', 'lol', 'omg', 'wtf', 'idk', 'fyi', 'tbh', 'lmao', 'asap', 'thanks', 'thank', 'thx', \n",
        "                'trav', 'bastante', 'muchisimo', 'muchisimos', 'muchisima', 'muchisimas', 'muchisimar', 'monton', 'habia', 'xe', 'jo', 'ops', 'ups', 'yupi', 'poca', 'poco', 'enhorabuena', 'incluso', 'igual', 'ahora', 'despu', 'pese', 'ser', 'cierto', 'haber', 'for', 'par', 'universidad', 'with', 'asimismo', 'carlos', 'universidad', 'iii', 'ser', 'entonces', 'madrid', 'tras', 'jo él', 'hacer', 'espán', 'noticia', 'as', 'periodico', 'diario', 'news', 'si', 'aca', 'mas', 'ademas', 'gracias', 'aqui', 'hola', 'saludos', 'hoy', 'co', 'bien', 'ver', 'dar', 'vosotrxs', 'wow', 'ahora', 'solo', 'todo', 'cualquier', 'cualquiera', 'pues', 'vez', 'mismo', 'evidentemente', 'uee', 'hey', 'hello', 'xvii', 'ii', 'iii', 'iv', 'v', 'vi', 'vii', 'viii', 'ix', 'x', 'xi', 'xii', 'xiii', 'xiv', 'xv', 'xvi', 'xviii', 'ixx', 'xx', 'xxi', \n",
        "                'parte', 'normalmente', 'mil', 'adio', 'millon', '!', 'yosoyurjc', 'haced', 'hacerte', 'mientras', 'menos', 'después', 'despues', 'dias', 'cuyo', 'cuya', 'of', '_', 'aun', 'nunca', 'siempre', 'muchisima', 'dia', 'algun', 'pon', 'sino', 'mejor', 'poca', 'peor', 'ano', 'nuevo', '200', '-', 'uee', 'href', 'uno', 'dos', 'tres', 'espán', 'medal él', 'desmontar él', 'jo él', 'joder', 'char él', 'queydondeestudiar', 'mer él', 'cuatro', 'cinco', 'seis', 'siete', 'ocho', 'nueve', 'diez', 'veintena', 'doce', 'docena', 'trece', 'veitiuno', 'cien', 'decena', 'x2', '–', 'https', 'http', 'rt', 'm', 'uc', 'alguien', 'cada', 'latest', 'tmb', 'via', 'tal', 'etc', 'etcetera', 'traves', 'ma', 'ahi', 'aqui', 'alli', 'alla', 'so', 'if', 'mmm', 'hmm', 'ja', 'cuidadosamente', 'quizas', 'quiza', 'nada', 'nunca', 'probablemente', 'mismisimo', 'totalmente', 'completamente'])\n",
        "\n",
        "stop_words.extend(['informacion', 'titulo', 'alumno', 'estudiar', 'profesor', 'primerisimo', 'adema', 'yomequedoencasa'])\n",
        "\n",
        "# Frequent words that add litle meaning: Hyperparameter in gridsearch\n",
        "extras = ['seguro', 'realizar', 'titulo', 'clase', 'tener', 'necesitar', 'querer', 'abrir', 'agarrar', 'andar', 'caminar', 'buscar', 'caer', 'conocer', 'saber', 'hablar', 'tener', 'tomar', 'poner', 'dar', 'ir', 'decir', 'estar', 'ser', 'vivir', 'oir', 'poner', 'traer', 'sentir', 'ver', 'mirar', 'llegar', 'llevar', 'entender', 'oler', 'salir', 'comprender', 'rayar', 'tener', 'flipar', 'putear', 'darle', 'saber', 'pasar', 'poner', 'esperar', 'dejar', 'parecer', 'salir', 'seguir', 'creer', 'opinar', 'informacion', 'alumno']\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}